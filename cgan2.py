# -*- coding: utf-8 -*-
"""CGAN2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C_SHtCNBWb828sHR77JYl9Ya8i12s2f-

The paper "Conditional Generative Adversarial Nets" by Mehdi Mirza and Simon Osindero introduces Conditional GANs (cGANs), which extend traditional GANs by conditioning the generator and discriminator on additional information (e.g., class labels or other modalities). Here's an overview of the paper and its relevance to your Free Space Optical (FSO) Communication project:

#Key Points from the Paper

##Generative Adversarial Networks (GANs):

GANs consist of two neural networks:
A generator (G) that tries to produce realistic samples.
A discriminator (D) that distinguishes between real and generated samples.
The generator and discriminator engage in a "min-max" game where they iteratively improve by trying to outsmart each other.

##Conditional GANs (cGANs):

In a cGAN, both the generator and discriminator are conditioned on extra information (denoted
ùë¶
y).
ùë¶
y could be class labels, partial data, or even data from other modalities.
This conditioning enables the generator to produce samples that adhere to specific requirements or characteristics.
Applications Discussed:

##Benefits of cGANs:

Enables controlled data generation based on specific conditions.
Can model multi-modal outputs (e.g., a single input can correspond to multiple valid outputs).

##Relevance to FSO Project
In Free Space Optical Communication, conditional data generation can be instrumental in solving challenges like simulating atmospheric conditions, optimizing signal transmission, or modeling dynamic environments. Here‚Äôs how cGANs can help:

###Simulating Atmospheric Conditions:

Condition the generator on specific parameters (e.g., fog density, turbulence strength, or weather types) to create realistic synthetic data representing optical signal behaviors under different conditions.
This data can supplement real-world measurements, which are often costly or difficult to obtain.

###Channel Modeling:

cGANs can be used to generate specific optical channel responses conditioned on input parameters like distance, wavelength, or aperture size.
These models can provide insight into system performance without requiring extensive physical testing.

###Data Augmentation:

If your dataset of optical signals under varying conditions is limited, cGANs can generate realistic augmented data conditioned on known attributes (e.g., power level, beam divergence).

###Anomaly Detection and Prediction:

By conditioning on past optical signal data, a cGAN could generate expected future signal patterns.
Discrepancies between actual and generated data could indicate anomalies or predict failures.


##How to Use cGANs for FSO Project
###Define Conditioning Variables:

Identify key parameters (e.g., atmospheric turbulence, signal-to-noise ratio, distance) that affect FSO communication and use them as conditioning variables
ùë¶
y.

###Prepare Data:

Organize your FSO dataset to include both input features (e.g., atmospheric data) and target outputs (e.g., signal degradation).

###Build a cGAN:

Design a generator to create synthetic FSO data conditioned on
ùë¶
y.
Train the discriminator to distinguish between real FSO data and generated data.

###Experiment and Evaluate:

Use the cGAN to simulate new scenarios or augment your dataset.
Validate the generated data against real-world measurements to ensure fidelity.
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/FSO/Sklavounos and Cohn, Autumn. 2022 .csv')

# Select relevant columns for inputs (features) and target
input_features = ['SolarFlux', 'H2Ocon', 'Tair', 'UmSonic', 'Pair']
target_variable = 'LogCn2'

# Normalize input features
scaler_x = MinMaxScaler()
X = df[input_features].values
X_scaled = scaler_x.fit_transform(X)

# Normalize the target variable
scaler_y = MinMaxScaler()
y = df[target_variable].values
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))

print("X_scaled shape:", X_scaled.shape)  # Features shape (samples, num_features)
print("y_scaled shape:", y_scaled.shape)  # Target shape (samples, 1)

import tensorflow as tf
from tensorflow.keras import layers, models

# Generator Model
def build_generator(input_dim, conditioning_dim, output_dim):
    model = models.Sequential()
    model.add(layers.InputLayer(input_shape=(input_dim + conditioning_dim,)))  # Noise + Conditioning Input
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.BatchNormalization())  # Normalize activations
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(32, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(output_dim))  # Single output for LogCn2
    return model

# Set dimensions
noise_dim = 10  # Random noise vector size
conditioning_dim = X_scaled.shape[1]  # Number of input features (5)
output_dim = 1  # Single target output

# Build the generator
generator = build_generator(noise_dim, conditioning_dim, output_dim)

# Print summary for verification
generator.summary()

# Discriminator Model
def build_discriminator(input_dim):
    model = models.Sequential()
    model.add(layers.InputLayer(input_shape=(input_dim,)))  # Combined input: X_scaled + LogCn2
    model.add(layers.Dense(128, activation='leaky_relu'))
    model.add(layers.Dropout(0.3))  # Prevent overfitting
    model.add(layers.Dense(64, activation='leaky_relu'))
    model.add(layers.Dropout(0.3))
    model.add(layers.Dense(32, activation='leaky_relu'))
    model.add(layers.Dense(1, activation='sigmoid'))  # Output: Real/Fake classification
    return model

# Input dimensions
discriminator_input_dim = X_scaled.shape[1] + output_dim  # Features + target

# Build the discriminator
discriminator = build_discriminator(discriminator_input_dim)

# Compile the discriminator
discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),
                      loss='binary_crossentropy',
                      metrics=['accuracy'])

# Print summary for verification
discriminator.summary()

# Combine Generator and Discriminator
def build_cgan(generator, discriminator):
    # Freeze the discriminator during generator training
    discriminator.trainable = False

    # CGAN model
    noise_input = layers.Input(shape=(noise_dim,))  # Noise input
    conditional_input = layers.Input(shape=(conditioning_dim,))  # Conditional input
    combined_input = layers.Concatenate()([noise_input, conditional_input])  # Combine inputs

    generated_output = generator(combined_input)  # Generator's output
    combined_data = layers.Concatenate()([conditional_input, generated_output])  # Combine with conditional input

    validity = discriminator(combined_data)  # Discriminator's output
    cgan_model = models.Model([noise_input, conditional_input], validity)  # CGAN model
    return cgan_model

# Build the CGAN
cgan = build_cgan(generator, discriminator)

# Compile the CGAN
cgan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),
             loss='binary_crossentropy')

# Print summary for verification
cgan.summary()

# Training parameters
epochs = 500
batch_size = 64
half_batch = batch_size // 2

# Store training history
d_losses = []
g_losses = []

for epoch in range(epochs):
    # ---------------------
    #  Train Discriminator
    # ---------------------
    # Select a random half batch of real data
    real_idx = np.random.randint(0, X_scaled.shape[0], half_batch)
    real_data = np.concatenate((X_scaled[real_idx], y_scaled[real_idx]), axis=1)
    real_labels = np.ones((half_batch, 1))  # Real labels are 1

    # Generate a half batch of fake data
    noise = np.random.randn(half_batch, noise_dim)
    gen_input = np.concatenate((noise, X_scaled[real_idx]), axis=1)
    y_fake = generator.predict(gen_input)  # Generator output
    fake_data = np.concatenate((X_scaled[real_idx], y_fake), axis=1)
    fake_labels = np.zeros((half_batch, 1))  # Fake labels are 0

    # Train the discriminator
    discriminator.trainable = True
    d_loss_real = discriminator.train_on_batch(real_data, real_labels)
    d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)  # Average loss
    discriminator.trainable = False

    # ---------------------
    #  Train Generator
    # ---------------------
    # Generate a full batch of noise
    noise = np.random.randn(batch_size, noise_dim)
    cond_idx = np.random.randint(0, X_scaled.shape[0], batch_size)
    conditional_data = X_scaled[cond_idx]

    # Train generator via CGAN
    valid_labels = np.ones((batch_size, 1))  # Generator tries to fool discriminator (labels = 1)
    g_loss = cgan.train_on_batch([noise, conditional_data], valid_labels)

    # Record the losses
    d_losses.append(d_loss[0])  # Store discriminator loss
    g_losses.append(g_loss)     # Store generator loss



    # Print progress
    if epoch % 100 == 0:
        d_loss_value = d_loss[0]
        d_acc = d_loss[1]
        # Access the first element of g_loss, which should be the loss value
        print(f"Epoch {epoch}/{epochs} | D Loss: {d_loss_value:.4f}, D Acc: {d_acc:.4f} | G Loss: {g_loss[0]:.4f}")

import matplotlib.pyplot as plt

plt.plot(d_losses, label='Discriminator Loss')
plt.plot(g_losses, label='Generator Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('CGAN Training Losses')
plt.show()

noise = np.random.randn(10, noise_dim)  # Generate 10 samples
conditional_data = X_scaled[:10]  # Use the first 10 rows for conditioning
generated_data = generator.predict(np.concatenate((noise, conditional_data), axis=1))
print("Generated Data:", generated_data)

# Generate new samples
num_samples = 10
noise = np.random.randn(num_samples, noise_dim)
conditional_inputs = X_scaled[:num_samples]  # Use the first few samples as conditions
gen_input = np.concatenate((noise, conditional_inputs), axis=1)

# Generate fake targets
fake_targets = generator.predict(gen_input)

# Rescale back to original scale
fake_targets_rescaled = scaler_y.inverse_transform(fake_targets)

# Print results
print("Generated Fake Targets (LogCn2):", fake_targets_rescaled)

generator.save('cgan_generator.h5')
discriminator.save('cgan_discriminator.h5')

import numpy as np
import matplotlib.pyplot as plt

# Example data, replace with your actual data
num_epochs = 500  # number of epochs
epochs = np.arange(num_epochs)  # Create a 1D array for epochs
g_loss = np.random.rand(num_epochs)  # Replace with actual generator loss values
d_loss = np.random.rand(num_epochs)  # Replace with actual discriminator loss values

# Create a figure and the first axis for generator loss
fig, ax1 = plt.subplots()

# Plot generator loss on the first axis (red line)
ax1.plot(epochs, g_loss, 'r-', label="Generator Loss")
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Generator Loss', color='r')
ax1.tick_params(axis='y', labelcolor='r')  # Set color of ticks to match line

# Create a second y-axis for discriminator loss
ax2 = ax1.twinx()

# Plot discriminator loss on the second axis (blue line)
ax2.plot(epochs, d_loss, 'b-', label="Discriminator Loss")
ax2.set_ylabel('Discriminator Loss', color='b')
ax2.tick_params(axis='y', labelcolor='b')  # Set color of ticks to match line

# Add title and labels
plt.title('CGAN Training Losses')

# Optional: If you want to have a legend
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')

# Display the plot
fig.tight_layout()  # Adjust layout to fit labels
plt.show()

import matplotlib.pyplot as plt

# Clear all existing figures
plt.close('all')

import matplotlib.pyplot as plt

# Plot discriminator and generator losses
plt.plot(d_losses, label='Discriminator Loss')  # Blue line for Discriminator Loss


# Add labels and title
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('CGAN Training Losses')



# Show the plot
plt.show()

import matplotlib.pyplot as plt



plt.plot(g_losses, label='Generator Loss', color='red')       # Red line for Generator Loss

# Add labels and title
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Generator Losses')

# Show the plot
plt.show()

print("Generator Losses (g_losses):", g_losses[:10])  # Check first 10 entries
print("Discriminator Losses (d_losses):", d_losses[:10])  # Check first 10 entries
print("Length of g_losses:", len(g_losses))
print("Length of d_losses:", len(d_losses))

import matplotlib.pyplot as plt

# Extract the first component from each g_losses entry
g_losses_flat = [entry[0] for entry in g_losses]

# Plot discriminator and generator losses
plt.plot(d_losses, label='Discriminator Loss', color='red')
plt.plot(g_losses_flat, label='Generator Loss', color='green')

# Add labels and title
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('CGAN Training Losses')

# Add legend
plt.legend()

# Show the plot
plt.show()

# Extract individual components
loss1 = [entry[0] for entry in g_losses]
loss2 = [entry[1] for entry in g_losses]
loss3 = [entry[2] for entry in g_losses]

# Plot each loss component
plt.plot(loss1, label='Generator Loss 1', color='red')
plt.plot(loss2, label='Generator Loss 2', color='green')
plt.plot(loss3, label='Generator Loss 3', color='orange')
plt.plot(d_losses, label='Discriminator Loss', color='blue')

# Add labels, title, and legend
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('CGAN Training Losses')
plt.legend()
plt.show()

print("Length of d_losses:", len(d_losses))
print("Length of g_losses:", len(g_losses))

