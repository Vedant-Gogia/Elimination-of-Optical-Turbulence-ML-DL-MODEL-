# -*- coding: utf-8 -*-
"""GAN in FSO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Yjtlhj7aHt6F1lpof4Vh-ow8mOfSMcR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import metrics

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/FSO/Sklavounos and Cohn, Autumn. 2022 .csv')

df.head()

df.isnull().sum()

df2=df.drop(['Date','Time'], axis=1)

corr=df2.corr()

#constructing heat map to inderstand correlation
plt.figure(figsize=(10,10))
sns.heatmap(corr,cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':10}, cmap='Blues')

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Separate features and target variable 'LogCn2'
X = df.drop(columns=['Date', 'Time', 'LogCn2'])
Y = df['LogCn2']

# Scale the features and target to a range suitable for GAN training (0 to 1)
scaler_X = MinMaxScaler(feature_range=(0, 1))
scaler_Y = MinMaxScaler(feature_range=(0, 1))

scaled_X = scaler_X.fit_transform(X)
scaled_Y = scaler_Y.fit_transform(Y.values.reshape(-1, 1))

# Combine scaled features and target for GAN input
scaled_data = np.hstack((scaled_X, scaled_Y))

# Check the shape of the preprocessed data
scaled_data.shape

scaled_Y

pip install tensorflow

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers

# Set dimensions for input and noise vector
input_dim = scaled_data.shape[1]  # Number of features (including 'LogCn2')
noise_dim = 100  # Dimension of noise vector for generator

# Define the Generator
def build_generator(noise_dim, output_dim):
    model = models.Sequential([
        layers.Dense(128, activation='relu', input_dim=noise_dim),
        layers.Dense(256, activation='relu'),
        layers.Dense(512, activation='relu'),
        layers.Dense(output_dim, activation='sigmoid')  # Output matches feature count
    ])
    return model

# Define the Discriminator
def build_discriminator(input_dim):
    model = models.Sequential([
        layers.Dense(512, activation='relu', input_dim=input_dim),
        layers.Dense(256, activation='relu'),
        layers.Dense(128, activation='relu'),
        layers.Dense(1, activation='sigmoid')  # Binary output (real/fake)
    ])
    return model

# Instantiate the Generator and Discriminator
generator = build_generator(noise_dim, input_dim)
discriminator = build_discriminator(input_dim)

# Compile the Discriminator
discriminator.compile(optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5),
                      loss='binary_crossentropy', metrics=['accuracy'])

# Combine models to form the GAN
discriminator.trainable = False  # Freeze discriminator in GAN model
gan_input = layers.Input(shape=(noise_dim,))
generated_data = generator(gan_input)
gan_output = discriminator(generated_data)
gan = models.Model(gan_input, gan_output)

# Compile the GAN
gan.compile(optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')

gan.summary()

import numpy as np

epochs = 3000  # Reduced number of epochs
batch_size = 128  # Increase batch size if memory allows
display_interval = 100  # More frequent updates to check progress

# Preprocessed dataset dimensions
real_data = scaled_data  # Combined features and target ('LogCn2')

for epoch in range(epochs):
    # ---- Train Discriminator ----
    # Select a random batch of real samples
    idx = np.random.randint(0, real_data.shape[0], batch_size)
    real_samples = real_data[idx]

    # Generate fake samples using the Generator
    noise = np.random.normal(0, 1, (batch_size, noise_dim))
    fake_samples = generator.predict(noise)

    # Label real as 1, fake as 0
    real_labels = np.ones((batch_size, 1))
    fake_labels = np.zeros((batch_size, 1))

# Train Discriminator
d_loss_real = discriminator.train_on_batch(real_samples, real_labels)
d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)
d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

print(d_loss_real)

print(d_loss_fake)

print(d_loss)

# ---- Train Generator ----
# Generate fake labels (label smoothing)
noise = np.random.normal(0, 1, (batch_size, noise_dim))
valid_labels = np.ones((batch_size, 1))

# Train GAN
g_loss = gan.train_on_batch(noise, valid_labels)

g_loss

# ---- Train Discriminator ----
d_loss_real = discriminator.train_on_batch(real_samples, real_labels)
d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)

# Extract the actual loss and accuracy values from lists
d_loss_real_value = d_loss_real[0]  # Loss from real samples
d_loss_fake_value = d_loss_fake[0]  # Loss from fake samples
d_accuracy_real = d_loss_real[1]  # Accuracy for real samples
d_accuracy_fake = d_loss_fake[1]  # Accuracy for fake samples

# Average the loss and accuracy for display
d_loss = 0.5 * (d_loss_real_value + d_loss_fake_value)
d_accuracy = 0.5 * (d_accuracy_real + d_accuracy_fake)

# ---- Train Generator ----
g_loss = gan.train_on_batch(noise, valid_labels)
g_loss_value = g_loss[0] if isinstance(g_loss, (list, tuple)) else g_loss  # Ensure `g_loss` is a single value

# Initialize lists to store metrics
d_losses, d_accuracies, g_losses = [], [], []

d_losses.append(d_loss)
d_accuracies.append(d_accuracy * 100)  # Convert to percentage for accuracy
g_losses.append(g_loss_value)

# Display progress
for epoch in range(epochs):
  if epoch % display_interval == 0:
    print(f"Epoch {epoch} in progress...")
    print(f"{epoch} [Discriminator loss: {d_loss:.4f}, accuracy: {100 * d_accuracy:.2f}%] [Generator loss: {g_loss_value:.4f}]")

# After training, save the models
generator.save("generator_model.h5")
discriminator.save("discriminator_model.h5")
gan.save("gan_model.h5")